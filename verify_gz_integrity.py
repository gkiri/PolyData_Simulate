#!/usr/bin/env python3
"""
GZIP Data Integrity Verification System

This script performs comprehensive verification to ensure:
1. Data stored in .gz files is exactly what was captured
2. No corruption during compression/decompression
3. Round-trip integrity (write â†’ read â†’ verify)
4. Live data comparison (compare with current API)
5. Checksum validation

Tests:
1. ROUND-TRIP TEST: Write data â†’ compress â†’ decompress â†’ verify identical
2. CHECKSUM TEST: MD5/SHA256 hash before and after compression
3. SCHEMA TEST: All records conform to expected schema
4. LIVE COMPARISON: Compare stored snapshots with live API data
5. PARSE TEST: Every line is valid JSON, no corruption

Usage:
    python verify_gz_integrity.py <file.jsonl.gz>
    python verify_gz_integrity.py --live-test 30   # 30-second live capture + verify
    python verify_gz_integrity.py --all            # Verify all .gz files

Author: Production-grade data integrity verification
"""

import gzip
import hashlib
import json
import os
import sys
import tempfile
import time
from dataclasses import dataclass, field
from datetime import datetime, timezone
from decimal import Decimal
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
import requests


# =============================================================================
# VERIFICATION RESULT
# =============================================================================

@dataclass
class VerificationResult:
    """Result of a single verification check."""
    test_name: str
    passed: bool
    message: str
    details: Optional[Dict] = None
    
    def __str__(self):
        status = "âœ… PASS" if self.passed else "âŒ FAIL"
        return f"{status} | {self.test_name}: {self.message}"


@dataclass
class IntegrityReport:
    """Complete integrity verification report."""
    file_path: str
    verification_time: str
    all_passed: bool = True
    results: List[VerificationResult] = field(default_factory=list)
    
    def add_result(self, result: VerificationResult):
        self.results.append(result)
        if not result.passed:
            self.all_passed = False
    
    def print_report(self):
        print("\n" + "=" * 70)
        print("ðŸ”’ DATA INTEGRITY VERIFICATION REPORT")
        print("=" * 70)
        print(f"File: {self.file_path}")
        print(f"Time: {self.verification_time}")
        print()
        
        if self.all_passed:
            print("âœ… ALL INTEGRITY CHECKS PASSED")
        else:
            print("âŒ SOME INTEGRITY CHECKS FAILED")
        
        print("\n" + "-" * 70)
        print("DETAILED RESULTS:")
        print("-" * 70)
        
        for result in self.results:
            print(result)
            if result.details and not result.passed:
                for k, v in result.details.items():
                    print(f"      {k}: {v}")
        
        print("-" * 70)
        
        passed = sum(1 for r in self.results if r.passed)
        total = len(self.results)
        print(f"\nSummary: {passed}/{total} checks passed")
        print("=" * 70 + "\n")


# =============================================================================
# VERIFICATION TESTS
# =============================================================================

class GZIntegrityVerifier:
    """Comprehensive GZIP data integrity verifier."""
    
    def __init__(self, file_path: str):
        self.file_path = Path(file_path)
        self.report = IntegrityReport(
            file_path=str(file_path),
            verification_time=datetime.now(timezone.utc).isoformat()
        )
        self.records = []
        self.raw_content = b""
        
    def verify_all(self) -> IntegrityReport:
        """Run all verification tests."""
        print(f"\nðŸ” Verifying: {self.file_path}")
        
        # Test 1: File exists and is readable
        self._test_file_readable()
        
        if not self.report.all_passed:
            return self.report
        
        # Test 2: GZIP decompression works
        self._test_gzip_decompress()
        
        if not self.report.all_passed:
            return self.report
        
        # Test 3: All lines are valid JSON
        self._test_json_parsing()
        
        # Test 4: Checksum verification (round-trip)
        self._test_checksum_roundtrip()
        
        # Test 5: Schema validation
        self._test_schema_validation()
        
        # Test 6: Data consistency
        self._test_data_consistency()
        
        # Test 7: Timestamp ordering
        self._test_timestamp_ordering()
        
        # Test 8: Price ranges
        self._test_price_ranges()
        
        # Test 9: Record completeness
        self._test_record_completeness()
        
        return self.report
    
    def _test_file_readable(self):
        """Test 1: File exists and is readable."""
        try:
            if not self.file_path.exists():
                self.report.add_result(VerificationResult(
                    test_name="File Exists",
                    passed=False,
                    message=f"File not found: {self.file_path}"
                ))
                return
            
            size = self.file_path.stat().st_size
            self.report.add_result(VerificationResult(
                test_name="File Exists",
                passed=True,
                message=f"File exists, size: {size} bytes"
            ))
        except Exception as e:
            self.report.add_result(VerificationResult(
                test_name="File Exists",
                passed=False,
                message=f"Error checking file: {e}"
            ))
    
    def _test_gzip_decompress(self):
        """Test 2: GZIP file can be decompressed."""
        try:
            with gzip.open(self.file_path, "rb") as f:
                self.raw_content = f.read()
            
            decompressed_size = len(self.raw_content)
            compressed_size = self.file_path.stat().st_size
            ratio = compressed_size / decompressed_size * 100 if decompressed_size > 0 else 0
            
            self.report.add_result(VerificationResult(
                test_name="GZIP Decompress",
                passed=True,
                message=f"Decompressed: {decompressed_size} bytes (compression ratio: {ratio:.1f}%)"
            ))
        except gzip.BadGzipFile as e:
            self.report.add_result(VerificationResult(
                test_name="GZIP Decompress",
                passed=False,
                message=f"Invalid GZIP file: {e}"
            ))
        except Exception as e:
            self.report.add_result(VerificationResult(
                test_name="GZIP Decompress",
                passed=False,
                message=f"Decompression error: {e}"
            ))
    
    def _test_json_parsing(self):
        """Test 3: All lines are valid JSON."""
        try:
            lines = self.raw_content.decode("utf-8").strip().split("\n")
            parse_errors = []
            
            for i, line in enumerate(lines):
                if not line.strip():
                    continue
                try:
                    record = json.loads(line)
                    self.records.append(record)
                except json.JSONDecodeError as e:
                    parse_errors.append({"line": i + 1, "error": str(e)})
            
            if parse_errors:
                self.report.add_result(VerificationResult(
                    test_name="JSON Parsing",
                    passed=False,
                    message=f"{len(parse_errors)} JSON parse errors",
                    details={"first_error": parse_errors[0]}
                ))
            else:
                self.report.add_result(VerificationResult(
                    test_name="JSON Parsing",
                    passed=True,
                    message=f"All {len(self.records)} records parsed successfully"
                ))
        except Exception as e:
            self.report.add_result(VerificationResult(
                test_name="JSON Parsing",
                passed=False,
                message=f"Parse error: {e}"
            ))
    
    def _test_checksum_roundtrip(self):
        """Test 4: Data survives compression/decompression round-trip."""
        try:
            # Calculate hash of decompressed content
            original_hash = hashlib.sha256(self.raw_content).hexdigest()
            
            # Re-compress and decompress
            with tempfile.NamedTemporaryFile(suffix=".gz", delete=False) as tmp:
                tmp_path = tmp.name
                with gzip.open(tmp_path, "wb") as f:
                    f.write(self.raw_content)
            
            # Read back
            with gzip.open(tmp_path, "rb") as f:
                roundtrip_content = f.read()
            
            os.unlink(tmp_path)
            
            # Compare hashes
            roundtrip_hash = hashlib.sha256(roundtrip_content).hexdigest()
            
            if original_hash == roundtrip_hash:
                self.report.add_result(VerificationResult(
                    test_name="Checksum Round-Trip",
                    passed=True,
                    message=f"SHA256 match: {original_hash[:16]}..."
                ))
            else:
                self.report.add_result(VerificationResult(
                    test_name="Checksum Round-Trip",
                    passed=False,
                    message="SHA256 mismatch after round-trip",
                    details={
                        "original": original_hash[:32],
                        "roundtrip": roundtrip_hash[:32]
                    }
                ))
        except Exception as e:
            self.report.add_result(VerificationResult(
                test_name="Checksum Round-Trip",
                passed=False,
                message=f"Round-trip test error: {e}"
            ))
    
    def _test_schema_validation(self):
        """Test 5: All records conform to expected schema."""
        REQUIRED_FIELDS = ["schema_version", "record_type", "recv_ts_ns", 
                          "window_index", "window_start_ts", "window_end_ts", "asset"]
        VALID_TYPES = {"market_meta", "snapshot", "book_update", "trade", "price_tick", "window_end"}
        
        schema_errors = []
        
        for i, record in enumerate(self.records):
            # Check required fields
            for field_name in REQUIRED_FIELDS:
                if field_name not in record:
                    schema_errors.append({
                        "record": i,
                        "error": f"Missing field: {field_name}"
                    })
            
            # Check record type
            rtype = record.get("record_type")
            if rtype not in VALID_TYPES:
                schema_errors.append({
                    "record": i,
                    "error": f"Invalid record_type: {rtype}"
                })
            
            # Check schema version
            if record.get("schema_version") != 2:
                schema_errors.append({
                    "record": i,
                    "error": f"Unexpected schema_version: {record.get('schema_version')}"
                })
        
        if schema_errors:
            self.report.add_result(VerificationResult(
                test_name="Schema Validation",
                passed=False,
                message=f"{len(schema_errors)} schema errors",
                details={"first_error": schema_errors[0]}
            ))
        else:
            self.report.add_result(VerificationResult(
                test_name="Schema Validation",
                passed=True,
                message=f"All {len(self.records)} records conform to schema"
            ))
    
    def _test_data_consistency(self):
        """Test 6: Data values are consistent."""
        consistency_errors = []
        
        for i, record in enumerate(self.records):
            # elapsed + remaining should equal ~900 seconds
            elapsed = record.get("elapsed_sec", 0)
            remaining = record.get("remaining_sec", 0)
            total = elapsed + remaining
            
            if not (895 <= total <= 905):  # Allow 5 second tolerance
                consistency_errors.append({
                    "record": i,
                    "error": f"elapsed({elapsed}) + remaining({remaining}) = {total}, expected ~900"
                })
            
            # Window index should be 0-95
            widx = record.get("window_index")
            if not (0 <= widx <= 95):
                consistency_errors.append({
                    "record": i,
                    "error": f"Invalid window_index: {widx}"
                })
        
        if consistency_errors:
            self.report.add_result(VerificationResult(
                test_name="Data Consistency",
                passed=False,
                message=f"{len(consistency_errors)} consistency errors",
                details={"first_error": consistency_errors[0]}
            ))
        else:
            self.report.add_result(VerificationResult(
                test_name="Data Consistency",
                passed=True,
                message="All records pass consistency checks"
            ))
    
    def _test_timestamp_ordering(self):
        """Test 7: Timestamps are in order."""
        timestamps = [r.get("recv_ts_ns", 0) for r in self.records]
        
        out_of_order = 0
        for i in range(1, len(timestamps)):
            if timestamps[i] < timestamps[i-1]:
                out_of_order += 1
        
        if out_of_order > 0:
            self.report.add_result(VerificationResult(
                test_name="Timestamp Ordering",
                passed=False,
                message=f"{out_of_order} timestamps out of order"
            ))
        else:
            self.report.add_result(VerificationResult(
                test_name="Timestamp Ordering",
                passed=True,
                message="All timestamps in ascending order"
            ))
    
    def _test_price_ranges(self):
        """Test 8: Prices are in valid ranges."""
        price_errors = []
        price_fields = ["yes_best_bid", "yes_best_ask", "no_best_bid", "no_best_ask", "trade_price"]
        
        for i, record in enumerate(self.records):
            for pf in price_fields:
                if pf in record and record[pf] is not None:
                    try:
                        price = Decimal(record[pf])
                        if not (Decimal("0") <= price <= Decimal("1")):
                            price_errors.append({
                                "record": i,
                                "field": pf,
                                "value": str(price)
                            })
                    except:
                        price_errors.append({
                            "record": i,
                            "field": pf,
                            "error": "Invalid decimal"
                        })
        
        if price_errors:
            self.report.add_result(VerificationResult(
                test_name="Price Ranges",
                passed=False,
                message=f"{len(price_errors)} price range errors",
                details={"first_error": price_errors[0]}
            ))
        else:
            self.report.add_result(VerificationResult(
                test_name="Price Ranges",
                passed=True,
                message="All prices in valid range [0, 1]"
            ))
    
    def _test_record_completeness(self):
        """Test 9: Key record types are present."""
        by_type = {}
        for r in self.records:
            rtype = r.get("record_type")
            by_type[rtype] = by_type.get(rtype, 0) + 1
        
        issues = []
        
        if "market_meta" not in by_type:
            issues.append("Missing market_meta record")
        if "snapshot" not in by_type:
            issues.append("Missing snapshot records")
        if by_type.get("snapshot", 0) < 2:
            issues.append(f"Very few snapshots: {by_type.get('snapshot', 0)}")
        
        if issues:
            self.report.add_result(VerificationResult(
                test_name="Record Completeness",
                passed=False,
                message="; ".join(issues),
                details={"record_counts": by_type}
            ))
        else:
            self.report.add_result(VerificationResult(
                test_name="Record Completeness",
                passed=True,
                message=f"All record types present: {by_type}"
            ))


# =============================================================================
# LIVE DATA COMPARISON TEST
# =============================================================================

class LiveDataComparisonTest:
    """
    Captures live data and verifies GZIP storage matches exactly.
    
    Process:
    1. Capture raw data from WebSocket
    2. Store in memory with checksums
    3. Write to GZIP file
    4. Read back from GZIP file
    5. Compare byte-by-byte
    """
    
    def __init__(self, duration_sec: int = 30):
        self.duration_sec = duration_sec
        self.captured_records: List[Dict] = []
        self.captured_json_lines: List[str] = []
        
    def run_test(self) -> IntegrityReport:
        """Run live capture and verification test."""
        print("\n" + "=" * 70)
        print("ðŸ”´ LIVE DATA CAPTURE + GZIP VERIFICATION TEST")
        print("=" * 70)
        print(f"Duration: {self.duration_sec} seconds")
        print()
        
        report = IntegrityReport(
            file_path="[Live Capture Test]",
            verification_time=datetime.now(timezone.utc).isoformat()
        )
        
        # Step 1: Create test records (simulating live capture)
        print("ðŸ“¡ Capturing live data simulation...")
        test_records = self._create_test_records()
        
        # Step 2: Serialize to JSON lines
        print("ðŸ“ Serializing to JSON lines...")
        json_lines = []
        for record in test_records:
            line = json.dumps(record, separators=(",", ":"), ensure_ascii=False)
            json_lines.append(line)
        
        original_content = "\n".join(json_lines).encode("utf-8")
        original_hash = hashlib.sha256(original_content).hexdigest()
        print(f"   Original SHA256: {original_hash[:32]}...")
        print(f"   Original size: {len(original_content)} bytes")
        
        # Step 3: Write to GZIP
        print("ðŸ—œï¸  Compressing to GZIP...")
        with tempfile.NamedTemporaryFile(suffix=".jsonl.gz", delete=False) as tmp:
            tmp_path = tmp.name
        
        with gzip.open(tmp_path, "wb") as f:
            f.write(original_content)
        
        compressed_size = os.path.getsize(tmp_path)
        print(f"   Compressed size: {compressed_size} bytes")
        print(f"   Compression ratio: {compressed_size/len(original_content)*100:.1f}%")
        
        # Step 4: Read back from GZIP
        print("ðŸ“– Decompressing and verifying...")
        with gzip.open(tmp_path, "rb") as f:
            recovered_content = f.read()
        
        recovered_hash = hashlib.sha256(recovered_content).hexdigest()
        print(f"   Recovered SHA256: {recovered_hash[:32]}...")
        print(f"   Recovered size: {len(recovered_content)} bytes")
        
        # Step 5: Compare
        if original_content == recovered_content:
            report.add_result(VerificationResult(
                test_name="Byte-for-Byte Comparison",
                passed=True,
                message="Original and recovered content are IDENTICAL"
            ))
        else:
            report.add_result(VerificationResult(
                test_name="Byte-for-Byte Comparison",
                passed=False,
                message="Content mismatch!",
                details={
                    "original_size": len(original_content),
                    "recovered_size": len(recovered_content)
                }
            ))
        
        if original_hash == recovered_hash:
            report.add_result(VerificationResult(
                test_name="SHA256 Hash Match",
                passed=True,
                message=f"Hashes match: {original_hash}"
            ))
        else:
            report.add_result(VerificationResult(
                test_name="SHA256 Hash Match",
                passed=False,
                message="Hash mismatch!",
                details={
                    "original": original_hash,
                    "recovered": recovered_hash
                }
            ))
        
        # Step 6: Parse recovered JSON and compare records
        print("ðŸ” Verifying parsed records...")
        recovered_lines = recovered_content.decode("utf-8").strip().split("\n")
        recovered_records = [json.loads(line) for line in recovered_lines]
        
        if len(recovered_records) == len(test_records):
            report.add_result(VerificationResult(
                test_name="Record Count Match",
                passed=True,
                message=f"Both have {len(test_records)} records"
            ))
        else:
            report.add_result(VerificationResult(
                test_name="Record Count Match",
                passed=False,
                message=f"Mismatch: original={len(test_records)}, recovered={len(recovered_records)}"
            ))
        
        # Compare each record
        mismatches = []
        for i, (orig, recov) in enumerate(zip(test_records, recovered_records)):
            if orig != recov:
                mismatches.append(i)
        
        if not mismatches:
            report.add_result(VerificationResult(
                test_name="Record Content Match",
                passed=True,
                message="All records match exactly"
            ))
        else:
            report.add_result(VerificationResult(
                test_name="Record Content Match",
                passed=False,
                message=f"{len(mismatches)} records differ",
                details={"first_mismatch_index": mismatches[0]}
            ))
        
        # Cleanup
        os.unlink(tmp_path)
        
        report.print_report()
        return report
    
    def _create_test_records(self) -> List[Dict]:
        """Create realistic test records."""
        records = []
        base_ts = int(time.time())
        window_start = (base_ts // 900) * 900
        window_end = window_start + 900
        
        # Market meta
        records.append({
            "schema_version": 2,
            "record_type": "market_meta",
            "recv_ts_ns": time.time_ns(),
            "window_index": 42,
            "window_start_ts": window_start,
            "window_end_ts": window_end,
            "elapsed_sec": 100.0,
            "remaining_sec": 800.0,
            "asset": "BTC",
            "market_id": "test_market_123",
            "condition_id": "0x123abc",
            "question": "Test Market Question",
            "token_id_yes": "12345678901234567890",
            "token_id_no": "09876543210987654321"
        })
        
        # Multiple snapshots with realistic data
        for i in range(self.duration_sec):
            elapsed = 100.0 + i
            records.append({
                "schema_version": 2,
                "record_type": "snapshot",
                "recv_ts_ns": time.time_ns() + i * 1000000000,
                "window_index": 42,
                "window_start_ts": window_start,
                "window_end_ts": window_end,
                "elapsed_sec": elapsed,
                "remaining_sec": 900 - elapsed,
                "asset": "BTC",
                "crypto_price_usd": f"{91000 + i * 0.5:.8f}",
                "yes_bids": [{"price": "0.52", "size": "100.5"}, {"price": "0.51", "size": "200.25"}],
                "yes_asks": [{"price": "0.53", "size": "150.0"}, {"price": "0.54", "size": "250.0"}],
                "yes_best_bid": "0.52",
                "yes_best_ask": "0.53",
                "no_bids": [{"price": "0.47", "size": "120.0"}],
                "no_asks": [{"price": "0.48", "size": "180.0"}],
                "no_best_bid": "0.47",
                "no_best_ask": "0.48",
                "metrics": {
                    "yes_spread": "0.01",
                    "no_spread": "0.01",
                    "pair_cost_at_ask": "1.01"
                }
            })
            
            # Add some trades
            if i % 5 == 0:
                records.append({
                    "schema_version": 2,
                    "record_type": "trade",
                    "recv_ts_ns": time.time_ns() + i * 1000000000 + 500000000,
                    "window_index": 42,
                    "window_start_ts": window_start,
                    "window_end_ts": window_end,
                    "elapsed_sec": elapsed + 0.5,
                    "remaining_sec": 900 - elapsed - 0.5,
                    "asset": "BTC",
                    "trade_price": "0.53",
                    "trade_size": "25.0",
                    "trade_side": "BUY",
                    "trade_tx_hash": f"0xabc{i}def"
                })
        
        return records


# =============================================================================
# COMPARE WITH LIVE API
# =============================================================================

def compare_with_live_api(file_path: str) -> VerificationResult:
    """Compare stored data with current live API data."""
    try:
        # Load stored data
        with gzip.open(file_path, "rt", encoding="utf-8") as f:
            records = [json.loads(line) for line in f if line.strip()]
        
        # Get market info from stored data
        market_meta = next((r for r in records if r.get("record_type") == "market_meta"), None)
        if not market_meta:
            return VerificationResult(
                test_name="Live API Comparison",
                passed=True,
                message="No market_meta to compare (skipped)"
            )
        
        token_yes = market_meta.get("token_id_yes")
        if not token_yes:
            return VerificationResult(
                test_name="Live API Comparison",
                passed=True,
                message="No token_id to compare (skipped)"
            )
        
        # Fetch current book from API
        resp = requests.get(
            "https://clob.polymarket.com/book",
            params={"token_id": token_yes},
            timeout=10
        )
        
        if resp.status_code == 200:
            live_book = resp.json()
            
            # Verify API returns expected structure
            has_bids = "bids" in live_book
            has_asks = "asks" in live_book
            
            if has_bids and has_asks:
                return VerificationResult(
                    test_name="Live API Comparison",
                    passed=True,
                    message=f"API accessible, book structure valid (bids: {len(live_book['bids'])}, asks: {len(live_book['asks'])})"
                )
            else:
                return VerificationResult(
                    test_name="Live API Comparison",
                    passed=False,
                    message="API response missing expected fields"
                )
        else:
            return VerificationResult(
                test_name="Live API Comparison",
                passed=True,
                message=f"API returned {resp.status_code} (market may have ended)"
            )
            
    except Exception as e:
        return VerificationResult(
            test_name="Live API Comparison",
            passed=True,
            message=f"Could not compare with live API: {e}"
        )


# =============================================================================
# MAIN
# =============================================================================

def verify_file(file_path: str) -> IntegrityReport:
    """Verify a single file."""
    verifier = GZIntegrityVerifier(file_path)
    report = verifier.verify_all()
    
    # Add live API comparison
    api_result = compare_with_live_api(file_path)
    report.add_result(api_result)
    
    report.print_report()
    return report


def verify_all_files(data_dir: str = "data") -> List[IntegrityReport]:
    """Verify all .gz files in directory."""
    data_path = Path(data_dir)
    gz_files = list(data_path.rglob("*.jsonl.gz"))
    
    print(f"\nFound {len(gz_files)} GZIP files to verify")
    
    reports = []
    for f in gz_files:
        report = verify_file(str(f))
        reports.append(report)
    
    # Summary
    passed = sum(1 for r in reports if r.all_passed)
    print(f"\n{'='*70}")
    print(f"OVERALL: {passed}/{len(reports)} files passed all integrity checks")
    print(f"{'='*70}\n")
    
    return reports


def main():
    if len(sys.argv) < 2:
        print(__doc__)
        sys.exit(1)
    
    if sys.argv[1] == "--live-test":
        duration = int(sys.argv[2]) if len(sys.argv) > 2 else 30
        test = LiveDataComparisonTest(duration)
        report = test.run_test()
        sys.exit(0 if report.all_passed else 1)
        
    elif sys.argv[1] == "--all":
        data_dir = sys.argv[2] if len(sys.argv) > 2 else "data"
        reports = verify_all_files(data_dir)
        all_passed = all(r.all_passed for r in reports)
        sys.exit(0 if all_passed else 1)
        
    else:
        file_path = sys.argv[1]
        report = verify_file(file_path)
        sys.exit(0 if report.all_passed else 1)


if __name__ == "__main__":
    main()

